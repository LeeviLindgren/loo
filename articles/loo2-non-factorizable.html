<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Leave-one-out cross-validation for non-factorizable models • loo</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><meta property="og:title" content="Leave-one-out cross-validation for non-factorizable models">
<meta property="og:description" content="">
<meta property="og:image" content="/logo.png">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">loo</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">2.0.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../articles/index.html">Vignettes</a>
</li>
<li>
  <a href="../reference/index.html">Functions</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Other Packages
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="http://mc-stan.org/rstan">rstan</a>
    </li>
    <li>
      <a href="http://mc-stan.org/rstanarm">rstanarm</a>
    </li>
    <li>
      <a href="http://mc-stan.org/bayesplot">bayesplot</a>
    </li>
    <li>
      <a href="http://mc-stan.org/shinystan">shinystan</a>
    </li>
    <li>
      <a href="http://mc-stan.org/rstantools">rstantools</a>
    </li>
  </ul>
</li>
<li>
  <a href="http://mc-stan.org">Stan</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://twitter.com/mcmc_stan">
    <span class="fa fa-twitter"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/stan-dev/loo">
    <span class="fa fa-github"></span>
     
  </a>
</li>
<li>
  <a href="http://discourse.mc-stan.org/">
    <span class="fa fa-users"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Leave-one-out cross-validation for non-factorizable models</h1>
                        <h4 class="author">Aki Vehtari, Paul Buerkner and Jonah Gabry</h4>
            
            <h4 class="date">2018-04-23</h4>
      
      

    </div>

    
    
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Leave-one-out cross-validation for non-factorizable models}
-->
<div id="introduction" class="section level1">
<h1 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h1>
<p>When computing approximate leave-one-out cross-validation (LOO-CV) after fitting a Bayesian model, the first step is to calculate the <em>pointwise</em> log-likelihood for every response value <span class="math inline">\(y_i, \: i = 1, \ldots, N\)</span>. This is straightforward for <em>factorizable</em> models in which response values are conditionally independent given the model parameters <span class="math inline">\(\theta\)</span> and the likelihood can be written in the familiar form</p>
<p><span class="math display">\[
p(y \,|\, \theta) = \prod_{i=1}^N p(y_i \,|\, \theta).
\]</span></p>
<p>The function <span class="math inline">\(p\)</span> will be either a probability density function (PDF) or a probability mass function (PMF) depending on whether we have a continuous or discrete outcome. When <span class="math inline">\(p(y)\)</span> can be factorized in this way, the conditional pointwise log-likelihood can be obtained easily by computing <span class="math inline">\(\log p(y_i \,|\, \theta)\)</span> for each <span class="math inline">\(i\)</span>. We then save each of these individual contributions to the log-likelihood rather than simply summing them to obtain the total log-likelihood.</p>
<p>The situation is more complicated for <em>non-factorizable</em> models in which response values are not conditionally independent. When there is residual dependency even after accounting for the model parameters <span class="math inline">\(\theta\)</span>, the conditional pointwise log-likelihood has the general form <span class="math inline">\(\log p(y_i \,|\, y_{-i}, \theta)\)</span>, where <span class="math inline">\(y_{-i}\)</span> denotes all response values except observation <span class="math inline">\(i\)</span>.</p>
</div>
<div id="loo-cv-for-multivariate-normal-models" class="section level1">
<h1 class="hasAnchor">
<a href="#loo-cv-for-multivariate-normal-models" class="anchor"></a>LOO-CV for multivariate normal models</h1>
<p>Although computing the pointwise log-likelihood for non-factorizable models is often impossible, there is a large class of multivariate normal models for which an analytical solution is available. These equations were initially derived by Sundararajan and Keerthi (2001) with a focus on the special case of a zero-mean Gaussian process model with prior covariance <span class="math inline">\(K\)</span> and residual standard deviation <span class="math inline">\(\sigma\)</span>,</p>
<p><span class="math display">\[
y \sim {\mathrm N}(0, \, K+\sigma^2 I),
\]</span></p>
<p>where <span class="math inline">\(I\)</span> is the identity matrix of appropriate dimension and <span class="math inline">\(C = K+\sigma^2 I\)</span> is the covariance matrix of the model.</p>
<p>Sundararajan and Keerthi’s results also generalize to the case of an arbitrary invertible covariance matrix <span class="math inline">\(C\)</span>. For such models, the LOO predictive mean and standard deviation can be computed as follows:</p>
<span class="math display">\[\begin{align}
  \mu_{\tilde{y},-i} &amp;= y_i-\bar{c}_{ii}^{-1} g_i \nonumber \\
  \sigma_{\tilde{y},-i} &amp;= \sqrt{\bar{c}_{ii}^{-1}},
\end{align}\]</span>
<p>where <span class="math inline">\(g_i\)</span> and <span class="math inline">\(\bar{c}_{ii}\)</span> are</p>
<span class="math display">\[\begin{align}
  g_i &amp;= \left[C^{-1} y\right]_i \nonumber \\
  \bar{c}_{ii} &amp;= \left[C^{-1}\right]_{ii}.
\end{align}\]</span>
<p>Using these results, the log predictive density of the <span class="math inline">\(i\)</span>th observation is then computed as</p>
<p><span class="math display">\[
  \log p(y_i \,|\, y_{-i},\theta)
  = - \frac{1}{2}\log(2\pi)
  - \frac{1}{2}\log \sigma^2_{-i}
  - \frac{1}{2}\frac{(y_i-\mu_{-i})^2}{\sigma^2_{-i}}.
\]</span></p>
<p>Expressing this same equation in terms of <span class="math inline">\(g_i\)</span> and <span class="math inline">\(\bar{c}_{ii}\)</span>, the log predictive density becomes:</p>
<p><span class="math display">\[
  \log p(y_i \,|\, y_{-i},\theta)
  = - \frac{1}{2}\log(2\pi)
  + \frac{1}{2}\log \bar{c}_{ii}
  - \frac{1}{2}\frac{g_i^2}{\bar{c}_{ii}}.
\]</span> (Note that Vehtari et al. (2016) has a typo in the corresponding Equation 34.)</p>
<p>From these equations we can now derive a recipe for obtaining the conditional pointwise log-likelihood for <em>all</em> models that can be expressed conditionally in terms of a multivariate normal with invertible covariance matrix <span class="math inline">\(C\)</span>.</p>
<div id="approximate-loo-cv-using-integrated-importance-sampling" class="section level2">
<h2 class="hasAnchor">
<a href="#approximate-loo-cv-using-integrated-importance-sampling" class="anchor"></a>Approximate LOO-CV using integrated importance-sampling</h2>
<p>The above LOO equations for multivariate normal models are conditional on parameters <span class="math inline">\(\theta\)</span>. Therefore, to obtain the leave-one-out predictive density <span class="math inline">\(p(y_i \,|\, y_{-i})\)</span> we need to integrate over <span class="math inline">\(\theta\)</span>,</p>
<p><span class="math display">\[
p(y_i\,|\,y_{-i}) =
  \int p(y_i\,|\,y_{-i}, \theta) \, p(\theta\,|\,y_{-i}) \,d\theta.
\]</span></p>
<p>Here, <span class="math inline">\(p(\theta\,|\,y_{-i})\)</span> is the leave-one-out posterior distribution for <span class="math inline">\(\theta\)</span>, that is, the posterior distribution for <span class="math inline">\(\theta\)</span> obtained by fitting the model while holding out the <span class="math inline">\(i\)</span>th observation.</p>
<p>To avoid the cost of sampling from <span class="math inline">\(N\)</span> leave-one-out posteriors, it is possible to take the posterior draws <span class="math inline">\(\theta^{(s)}, \, s=1,\ldots,S\)</span>, from the  posterior <span class="math inline">\(p(\theta\,|\,y)\)</span>, and then approximate the above integral using integrated importance sampling (Vehtari et al., 2016, Section 3.6.1):</p>
<p><span class="math display">\[
 p(y_i\,|\,y_{-i}) \approx
   \frac{ \sum_{s=1}^S p(y_i\,|\,y_{-i},\,\theta^{(s)}) \,w_i^{(s)}}{ \sum_{s=1}^S w_i^{(s)}},
\]</span></p>
<p>where <span class="math inline">\(w_i^{(s)}\)</span> are importance weights. First we compute the raw importance ratios</p>
<p><span class="math display">\[
  r_i^{(s)} \propto \frac{1}{p(y_i \,|\, y_{-i}, \,\theta^{(s)})},
\]</span></p>
<p>and then stabilize them using Pareto smoothed importance sampling (PSIS, Vehtari et al, 2017b) to obtain the weights <span class="math inline">\(w_i^{(s)}\)</span>. The resulting approximation is referred to as PSIS-LOO (Vehtari et al, 2017a).</p>
</div>
<div id="exact-loo-cv-with-re-fitting" class="section level2">
<h2 class="hasAnchor">
<a href="#exact-loo-cv-with-re-fitting" class="anchor"></a>Exact LOO-CV with re-fitting</h2>
<p>In order to validate the approximate LOO procedure, and also in order to allow exact computations to be made for a small number of leave-one-out folds for which the Pareto <span class="math inline">\(k\)</span> diagnostic (Vehtari et al, 2017b) indicates an unstable approximation, we need to consider how we might to do <em>exact</em> leave-one-out CV for a non-factorizable model. In the case of a Gaussian process that has the marginalization property, we could just drop the one row and column of <span class="math inline">\(C\)</span> corresponding to the held out out observation. This does not hold in general for multivariate normal models, however, and to keep the original prior we may need to maintain the full covariance matrix <span class="math inline">\(C\)</span> even when one of the observations is left out.</p>
<p>The solution is to model <span class="math inline">\(y_i\)</span> as a missing observation and estimate it along with all of the other model parameters. For a conditional multivariate normal model, <span class="math inline">\(\log p(y_i\,|\,y_{-i})\)</span> can be computed as follows. First, we model <span class="math inline">\(y_i\)</span> as missing and denote the corresponding parameter <span class="math inline">\(y_i^{\mathrm{mis}}\)</span>. Then, we define</p>
<p><span class="math display">\[
y_{\mathrm{mis}(i)} = (y_1, \ldots, y_{i-1}, y_i^{\mathrm{mis}}, y_{i+1}, \ldots, y_N).
\]</span> to be the same as the full set of observations <span class="math inline">\(y\)</span>, except replacing <span class="math inline">\(y_i\)</span> with the parameter <span class="math inline">\(y_i^{\mathrm{mis}}\)</span>.</p>
<p>Second, we compute the LOO predictive mean and standard deviations as above, but replace <span class="math inline">\(y\)</span> with <span class="math inline">\(y_{\mathrm{mis}(i)}\)</span> in the computation of <span class="math inline">\(\mu_{\tilde{y},-i}\)</span>:</p>
<p><span class="math display">\[
\mu_{\tilde{y},-i} = y_{{\mathrm{mis}}(i)}-\bar{c}_{ii}^{-1}g_i,
\]</span></p>
<p>where in this case we have</p>
<p><span class="math display">\[
g_i = \left[ C^{-1} y_{\mathrm{mis}(i)} \right]_i.
\]</span></p>
<p>The conditional log predictive density is then computed with the above <span class="math inline">\(\mu_{\tilde{y},-i}\)</span> and the left out observation <span class="math inline">\(y_i\)</span>:</p>
<p><span class="math display">\[
  \log p(y_i\,|\,y_{-i},\theta)
  = - \frac{1}{2}\log(2\pi)
  - \frac{1}{2}\log \sigma^2_{\tilde{y},-i}
  - \frac{1}{2}\frac{(y_i-\mu_{\tilde{y},-i})^2}{\sigma^2_{\tilde{y},-i}}.
\]</span></p>
<p>Finally, the leave-one-out predictive distribution can then be estimated as</p>
<p><span class="math display">\[
 p(y_i\,|\,y_{-i}) \approx \sum_{s=1}^S p(y_i\,|\,y_{-i}, \theta_{-i}^{(s)}),
\]</span></p>
<p>where <span class="math inline">\(\theta_{-i}^{(s)}\)</span> are draws from the posterior distribution <span class="math inline">\(p(\theta\,|\,y_{\mathrm{mis}(i)})\)</span>.</p>
</div>
</div>
<div id="lagged-sar-models" class="section level1">
<h1 class="hasAnchor">
<a href="#lagged-sar-models" class="anchor"></a>Lagged SAR models</h1>
<p>A common non-factorizable multivariate normal model is the simultaneously autoregressive (SAR) model, which is frequently used for spatially correlated data. The lagged SAR model is defined as</p>
<p><span class="math display">\[
y = \rho Wy + \eta + \epsilon
\]</span> or equivalently <span class="math display">\[
(I - \rho W)y = \eta + \epsilon,
\]</span> where <span class="math inline">\(\rho\)</span> is the spatial correlation parameter and <span class="math inline">\(W\)</span> is a user-defined weight matrix. The matrix <span class="math inline">\(W\)</span> has entries <span class="math inline">\(w_{ii} = 0\)</span> along the diagonal and the off-diagonal entries <span class="math inline">\(w_{ij}\)</span> are larger when areas <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are closer to each other. In a linear model, the predictor term <span class="math inline">\(\eta\)</span> is given by <span class="math inline">\(\eta = X \beta\)</span> with design matrix <span class="math inline">\(X\)</span> and regression coefficients <span class="math inline">\(\beta\)</span>. However, since the above equation holds for arbitrary <span class="math inline">\(\eta\)</span>, these results are not restricted to linear models.</p>
<p>If we have <span class="math inline">\(\epsilon \sim {\mathrm N}(0, \,\sigma^2 I)\)</span>, it follows that <span class="math display">\[
(I - \rho W)y \sim {\mathrm N}(\eta, \sigma^2 I),
\]</span> which corresponds to the following log PDF coded in <strong>Stan</strong>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">/</span><span class="er">**</span><span class="st"> </span>
<span class="st"> </span><span class="er">*</span><span class="st"> </span>Normal log<span class="op">-</span>pdf <span class="cf">for</span> spatially lagged responses
 <span class="op">*</span><span class="st"> </span>
<span class="st"> </span><span class="er">*</span><span class="st"> </span><span class="er">@</span>param y Vector of response values.
 <span class="op">*</span><span class="st"> </span><span class="er">@</span>param mu Mean parameter vector.
 <span class="op">*</span><span class="st"> </span><span class="er">@</span>param sigma Positive scalar residual standard deviation.
 <span class="op">*</span><span class="st"> </span><span class="er">@</span>param rho Positive scalar autoregressive parameter.
 <span class="op">*</span><span class="st"> </span><span class="er">@</span>param W Spatial weight matrix.
 <span class="op">*</span>
<span class="st"> </span><span class="er">*</span><span class="st"> </span><span class="er">@</span>return A scalar to be added to the log posterior.
 <span class="op">*</span><span class="er">/</span>
real <span class="kw">normal_lagsar_lpdf</span>(vector y, vector mu, real sigma, 
                        real rho, matrix W) {
  int N =<span class="st"> </span><span class="kw">rows</span>(y);
  real inv_sigma2 =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">square</span>(sigma);
  matrix[N, N] W_tilde =<span class="st"> </span><span class="op">-</span>rho <span class="op">*</span><span class="st"> </span>W;
  vector[N] half_pred;
  
  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) W_tilde[n,n] <span class="op">+</span><span class="er">=</span><span class="st"> </span><span class="dv">1</span>;
  
  half_pred =<span class="st"> </span>W_tilde <span class="op">*</span><span class="st"> </span>(y <span class="op">-</span><span class="st"> </span><span class="kw">mdivide_left</span>(W_tilde, mu));
  
  return <span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">log_determinant</span>(<span class="kw">crossprod</span>(W_tilde) <span class="op">*</span><span class="st"> </span>inv_sigma2) <span class="op">-</span>
<span class="st">         </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">dot_self</span>(half_pred) <span class="op">*</span><span class="st"> </span>inv_sigma2;
}</code></pre></div>
<p>For the purpose of computing LOO-CV, it makes sense to rewrite the SAR model in slightly different form. Conditional on <span class="math inline">\(\rho\)</span>, <span class="math inline">\(\eta\)</span>, and <span class="math inline">\(\sigma\)</span>, if we write</p>
<span class="math display">\[\begin{align}
y-(I-\rho W)^{-1}\eta &amp;\sim {\mathrm N}(0, \sigma^2(I-\rho W)^{-1}(I-\rho W)^{-T}),
\end{align}\]</span>
<p>or more compactly, with <span class="math inline">\(\widetilde{W}=(I-\rho W)\)</span>,</p>
<span class="math display">\[\begin{align}
y-\widetilde{W}^{-1}\eta &amp;\sim {\mathrm N}(0, \sigma^2(\widetilde{W}^{T}\widetilde{W})^{-1}),
\end{align}\]</span>
<p>then this has the same form as the zero mean Gaussian process from above. Accordingly, we can compute the leave-one-out predictive densities with the equations from Sundararajan and Keerthi (2001), replacing <span class="math inline">\(y\)</span> with <span class="math inline">\((y-\widetilde{W}^{-1}\eta)\)</span> and taking the covariance matrix <span class="math inline">\(C\)</span> to be <span class="math inline">\(\sigma^2(\widetilde{W}^{T}\widetilde{W})^{-1}\)</span>.</p>
<div id="case-study-neighborhood-crime-in-columbus-ohio" class="section level2">
<h2 class="hasAnchor">
<a href="#case-study-neighborhood-crime-in-columbus-ohio" class="anchor"></a>Case Study: Neighborhood Crime in Columbus, Ohio</h2>
<p>In order to demonstrate how to carry out the computations implied by these equations, we will first fit a lagged SAR model to data on crime in 49 different neighborhoods of Columbus, Ohio during the year 1980. The data was originally described in Aneslin (1988) and ships with the <strong>spdep</strong> R package.</p>
<p>In addition to the <strong>loo</strong> package, for this analysis we will use the <strong>brms</strong> interface to Stan to generate a Stan program and fit the model, and also the <strong>bayesplot</strong> and <strong>ggplot2</strong> packages for plotting.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">"loo"</span>)
<span class="kw">library</span>(<span class="st">"brms"</span>)
<span class="kw">library</span>(<span class="st">"bayesplot"</span>)
<span class="kw">library</span>(<span class="st">"ggplot2"</span>)
<span class="kw"><a href="http://www.rdocumentation.org/packages/bayesplot/topics/bayesplot-colors">color_scheme_set</a></span>(<span class="st">"brightblue"</span>)
<span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/theme_get">theme_set</a></span>(<span class="kw"><a href="http://www.rdocumentation.org/packages/brms/topics/theme_default">theme_default</a></span>())


SEED &lt;-<span class="st"> </span><span class="dv">10001</span> 
<span class="kw">set.seed</span>(SEED) <span class="co"># only sets seed for R (seed for Stan set later)</span>

<span class="co"># loads COL.OLD data frame and COL.nb neighbor list</span>
<span class="kw">data</span>(oldcol, <span class="dt">package =</span> <span class="st">"spdep"</span>) </code></pre></div>
<p>The three variables in the data set relevant to this example are:</p>
<ul>
<li>
<code>CRIME</code>: the number of residential burglaries and vehicle thefts per thousand households in the neighbood</li>
<li>
<code>HOVAL</code>: housing value in units of $1000 USD</li>
<li>
<code>INC</code>: household income in units of $1000 USD</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(COL.OLD[, <span class="kw">c</span>(<span class="st">"CRIME"</span>, <span class="st">"HOVAL"</span>, <span class="st">"INC"</span>)])</code></pre></div>
<pre><code>'data.frame':   49 obs. of  3 variables:
 $ CRIME: num  18.802 32.388 38.426 0.178 15.726 ...
 $ HOVAL: num  44.6 33.2 37.1 75 80.5 ...
 $ INC  : num  21.23 4.48 11.34 8.44 19.53 ...</code></pre>
<p>We will also use the object <code>COL.nb</code>, which is a list containing information about which neighborhoods border each other. From this list we will be able to construct the weight matrix to used to help account for the spatial dependency among the observations.</p>
<div id="fit-lagged-sar-model" class="section level3">
<h3 class="hasAnchor">
<a href="#fit-lagged-sar-model" class="anchor"></a>Fit lagged SAR model</h3>
<p>A model predicting <code>CRIME</code> from <code>INC</code> and <code>HOVAL</code>, while accounting for the spatial dependency via an SAR structure, can be specified in <strong>brms</strong> as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/brms/topics/brm">brm</a></span>(
  CRIME <span class="op">~</span><span class="st"> </span>INC <span class="op">+</span><span class="st"> </span>HOVAL, 
  <span class="dt">data =</span> COL.OLD,
  <span class="dt">autocor =</span> <span class="kw"><a href="http://www.rdocumentation.org/packages/brms/topics/cor_sar">cor_lagsar</a></span>(COL.nb),
  <span class="dt">chains =</span> <span class="dv">4</span>,
  <span class="dt">seed =</span> SEED
)</code></pre></div>
<p>The code above fits the model in <strong>Stan</strong> using a log PDF equivalent to the <code>normal_lagsar_lpdf</code> function we defined above. In the summary output below we see that both higher income and higher housing value predict lower crime rates in the neighborhood. Moreover, there seems to be substantial spatial correlation between adjacent neighborhoods, as indicated by the posterior distribution of the <code>lagsar</code> parameter.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lagsar &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(fit, <span class="dt">pars =</span> <span class="st">"lagsar"</span>)
estimates &lt;-<span class="st"> </span><span class="kw">quantile</span>(lagsar, <span class="dt">probs =</span> <span class="kw">c</span>(<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>))
<span class="kw"><a href="http://www.rdocumentation.org/packages/bayesplot/topics/MCMC-distributions">mcmc_hist</a></span>(lagsar) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/bayesplot/topics/bayesplot-helpers">vline_at</a></span>(estimates, <span class="dt">linetype =</span> <span class="dv">2</span>, <span class="dt">size =</span> <span class="dv">1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/labs">ggtitle</a></span>(<span class="st">"lagsar: posterior median and 50% central interval"</span>)</code></pre></div>
<p><img src="loo2-non-factorizable_files/figure-html/lagsar-1.png" width="60%" style="display: block; margin: auto;"></p>
</div>
<div id="approximate-loo-cv" class="section level3">
<h3 class="hasAnchor">
<a href="#approximate-loo-cv" class="anchor"></a>Approximate LOO-CV</h3>
<p>After fitting the model, the next step is to compute the pointwise log-likelihood values needed for approximate LOO-CV. To do this we will use the recipe laid out in the previous sections.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">posterior &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit)
y &lt;-<span class="st"> </span>fit<span class="op">$</span>data<span class="op">$</span>CRIME
N &lt;-<span class="st"> </span><span class="kw">length</span>(y)
S &lt;-<span class="st"> </span><span class="kw">nrow</span>(posterior)
loglik &lt;-<span class="st"> </span>yloo &lt;-<span class="st"> </span>sdloo &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> S, <span class="dt">ncol =</span> N)

<span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>S) {
  p &lt;-<span class="st"> </span>posterior[s, ]
  eta &lt;-<span class="st"> </span>p<span class="op">$</span>b_Intercept <span class="op">+</span><span class="st"> </span>p<span class="op">$</span>b_INC <span class="op">*</span><span class="st"> </span>fit<span class="op">$</span>data<span class="op">$</span>INC <span class="op">+</span><span class="st"> </span>p<span class="op">$</span>b_HOVAL <span class="op">*</span><span class="st"> </span>fit<span class="op">$</span>data<span class="op">$</span>HOVAL
  W_tilde &lt;-<span class="st"> </span><span class="kw">diag</span>(N) <span class="op">-</span><span class="st"> </span>p<span class="op">$</span>lagsar <span class="op">*</span><span class="st"> </span>fit<span class="op">$</span>autocor<span class="op">$</span>W
  Cinv &lt;-<span class="st"> </span><span class="kw">t</span>(W_tilde) <span class="op">%*%</span><span class="st"> </span>W_tilde <span class="op">/</span><span class="st"> </span>p<span class="op">$</span>sigma<span class="op">^</span><span class="dv">2</span>
  g &lt;-<span class="st"> </span>Cinv <span class="op">%*%</span><span class="st"> </span>(y <span class="op">-</span><span class="st"> </span><span class="kw">solve</span>(W_tilde, eta))
  cbar &lt;-<span class="st"> </span><span class="kw">diag</span>(Cinv)
  yloo[s, ] &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span>g <span class="op">/</span><span class="st"> </span>cbar
  sdloo[s, ] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>cbar)
  loglik[s, ] &lt;-<span class="st"> </span><span class="kw">dnorm</span>(y, yloo[s, ], sdloo[s, ], <span class="dt">log =</span> <span class="ot">TRUE</span>)
}

<span class="co"># use loo for psis smoothing</span>
log_ratios &lt;-<span class="st"> </span><span class="op">-</span>loglik
psis_result &lt;-<span class="st"> </span><span class="kw"><a href="../reference/psis.html">psis</a></span>(log_ratios)</code></pre></div>
<p>The quality of the PSIS-LOO approximation can be investigated graphically by plotting the Pareto-k estimate for each observation. Ideally, they should not exceed <span class="math inline">\(0.5\)</span>, but in practice the algorithm turns out to be robust up to values of <span class="math inline">\(0.7\)</span> (Vehtari et al, 2017ab). In the plot below, we see that the fourth observation is problematic and so may reduce the accuracy of the LOO-CV approximation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(psis_result, <span class="dt">label_points =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="loo2-non-factorizable_files/figure-html/unnamed-chunk-5-1.png" width="60%" style="display: block; margin: auto;"></p>
<p>We can also check that the conditional leave-one-out predictive distribution equations work correctly, for instance, using the last posterior draw:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">yloo_sub &lt;-<span class="st"> </span>yloo[S, ]
sdloo_sub &lt;-<span class="st"> </span>sdloo[S, ]
df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">y =</span> y, 
  <span class="dt">yloo =</span> yloo_sub,
  <span class="dt">ymin =</span> yloo_sub <span class="op">-</span><span class="st"> </span>sdloo_sub <span class="op">*</span><span class="st"> </span><span class="dv">2</span>,
  <span class="dt">ymax =</span> yloo_sub <span class="op">+</span><span class="st"> </span>sdloo_sub <span class="op">*</span><span class="st"> </span><span class="dv">2</span>
)
<span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/ggplot">ggplot</a></span>(<span class="dt">data=</span>df, <span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/aes">aes</a></span>(<span class="dt">x =</span> y, <span class="dt">y =</span> yloo, <span class="dt">ymin =</span> ymin, <span class="dt">ymax =</span> ymax)) <span class="op">+</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/geom_linerange">geom_errorbar</a></span>(
    <span class="dt">width =</span> <span class="dv">1</span>, 
    <span class="dt">color =</span> <span class="st">"skyblue3"</span>, 
    <span class="dt">position =</span> <span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/position_jitter">position_jitter</a></span>(<span class="dt">width =</span> <span class="fl">0.25</span>)
  ) <span class="op">+</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/geom_abline">geom_abline</a></span>(<span class="dt">color =</span> <span class="st">"gray30"</span>, <span class="dt">size =</span> <span class="fl">1.2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/geom_point">geom_point</a></span>()</code></pre></div>
<p><img src="loo2-non-factorizable_files/figure-html/unnamed-chunk-6-1.png" width="60%" style="display: block; margin: auto;"></p>
<p>Finally, we use PSIS-LOO to approximate the expected log predictive density (ELPD) for new data, which we will validate using exact LOO-CV in the upcoming section.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(psis_loo &lt;-<span class="st"> </span><span class="kw"><a href="../reference/loo.html">loo</a></span>(loglik))</code></pre></div>
<pre><code>
Computed from 4000 by 49 log-likelihood matrix

         Estimate   SE
elpd_loo   -187.8 11.7
p_loo         8.9  6.1
looic       375.6 23.5
------
Monte Carlo SE of elpd_loo is NA.

Pareto k diagnostic values:
                         Count Pct.    Min. n_eff
(-Inf, 0.5]   (good)     48    98.0%   678       
 (0.5, 0.7]   (ok)        0     0.0%   &lt;NA&gt;      
   (0.7, 1]   (bad)       0     0.0%   &lt;NA&gt;      
   (1, Inf)   (very bad)  1     2.0%   6         
See help('pareto-k-diagnostic') for details.</code></pre>
</div>
<div id="exact-loo-cv" class="section level3">
<h3 class="hasAnchor">
<a href="#exact-loo-cv" class="anchor"></a>Exact LOO-CV</h3>
<p>Exact LOO-CV for the above example is somewhat more involved, as we need to re-fit the model <span class="math inline">\(N\)</span> times and each time model the held-out data point as a parameter. First, we create an empty dummy model that we will update below as we loop over the observations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># see help("mi", "brms") for details on the mi() usage</span>
fit_dummy &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/brms/topics/brm">brm</a></span>(
  CRIME <span class="op">|</span><span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/brms/topics/mi">mi</a></span>() <span class="op">~</span><span class="st"> </span>INC <span class="op">+</span><span class="st"> </span>HOVAL, 
  <span class="dt">data =</span> COL.OLD,
  <span class="dt">autocor =</span> <span class="kw"><a href="http://www.rdocumentation.org/packages/brms/topics/cor_sar">cor_lagsar</a></span>(COL.nb), 
  <span class="dt">chains =</span> <span class="dv">0</span>
)</code></pre></div>
<p>Next, we fit the model <span class="math inline">\(N\)</span> times, each time leaving out a single observation and then computing the log predictive density for that observation. For obvious reasons, this takes much longer than the approximation we computed above, but it is necessary in order to validate the approximate LOO-CV method. Thanks to the PSIS-LOO approximation, in general doing these slow exact computations can be avoided.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">S &lt;-<span class="st"> </span><span class="dv">500</span>
res &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">"list"</span>, N)
loglik &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">nrow =</span> S, <span class="dt">ncol =</span> N)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(N)) {
  dat_mi &lt;-<span class="st"> </span>COL.OLD
  dat_mi<span class="op">$</span>CRIME[i] &lt;-<span class="st"> </span><span class="ot">NA</span>
  fit_i &lt;-<span class="st"> </span><span class="kw">update</span>(fit_dummy, <span class="dt">newdata =</span> dat_mi, 
                  <span class="co"># just for vignette</span>
                  <span class="dt">chains =</span> <span class="dv">1</span>, <span class="dt">iter =</span> S <span class="op">*</span><span class="st"> </span><span class="dv">2</span>)
  posterior &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fit_i)
  yloo &lt;-<span class="st"> </span>sdloo &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, S)
  <span class="cf">for</span> (s <span class="cf">in</span> <span class="kw">seq_len</span>(S)) {
    p &lt;-<span class="st"> </span>posterior[s, ]
    y_miss_i &lt;-<span class="st"> </span>y
    y_miss_i[i] &lt;-<span class="st"> </span>p<span class="op">$</span>Ymi
    eta &lt;-<span class="st"> </span>p<span class="op">$</span>b_Intercept <span class="op">+</span><span class="st"> </span>p<span class="op">$</span>b_INC <span class="op">*</span><span class="st"> </span>fit_i<span class="op">$</span>data<span class="op">$</span>INC <span class="op">+</span><span class="st"> </span>p<span class="op">$</span>b_HOVAL <span class="op">*</span><span class="st"> </span>fit_i<span class="op">$</span>data<span class="op">$</span>HOVAL
    W_tilde &lt;-<span class="st"> </span><span class="kw">diag</span>(N) <span class="op">-</span><span class="st"> </span>p<span class="op">$</span>lagsar <span class="op">*</span><span class="st"> </span>fit_i<span class="op">$</span>autocor<span class="op">$</span>W
    Cinv &lt;-<span class="st"> </span><span class="kw">t</span>(W_tilde) <span class="op">%*%</span><span class="st"> </span>W_tilde <span class="op">/</span><span class="st"> </span>p<span class="op">$</span>sigma<span class="op">^</span><span class="dv">2</span>
    g &lt;-<span class="st"> </span>Cinv <span class="op">%*%</span><span class="st"> </span>(y_miss_i <span class="op">-</span><span class="st"> </span><span class="kw">solve</span>(W_tilde, eta))
    cbar &lt;-<span class="st"> </span><span class="kw">diag</span>(Cinv);
    yloo[s] &lt;-<span class="st"> </span>y_miss_i[i] <span class="op">-</span><span class="st"> </span>g[i] <span class="op">/</span><span class="st"> </span>cbar[i]
    sdloo[s] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>cbar[i])
    loglik[s, i] &lt;-<span class="st"> </span><span class="kw">dnorm</span>(y[i], yloo[s], sdloo[s], <span class="dt">log =</span> <span class="ot">TRUE</span>)
  }
  ypred &lt;-<span class="st"> </span><span class="kw">rnorm</span>(S, yloo, sdloo)
  res[[i]] &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> <span class="kw">c</span>(posterior<span class="op">$</span>Ymi, ypred))
  res[[i]]<span class="op">$</span>type &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">c</span>(<span class="st">"pp"</span>, <span class="st">"loo"</span>), <span class="dt">each =</span> S)
  res[[i]]<span class="op">$</span>obs &lt;-<span class="st"> </span>i
}
res &lt;-<span class="st"> </span><span class="kw">do.call</span>(rbind, res)</code></pre></div>
<p>A first step in the validation of the pointwise predictive density is to compare the distribution of the implied response values for the left-out observation to the distribution of the <span class="math inline">\(y_i^{\mathrm{mis}}\)</span> posterior-predictive values estimated as part of the model. If the pointwise predictive density is correct, the two distributions should match very closely (up to sampling error). In the plot below, we overlay these two distributions for the first four observations and see that they match very closely (as is the case for all <span class="math inline">\(49\)</span> observations of in this example).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res_sub &lt;-<span class="st"> </span>res[res<span class="op">$</span>obs <span class="op">%in%</span><span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, ]
<span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/ggplot">ggplot</a></span>(res_sub, <span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/aes">aes</a></span>(y, <span class="dt">fill =</span> type)) <span class="op">+</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/geom_density">geom_density</a></span>(<span class="dt">alpha =</span> <span class="fl">0.6</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/facet_wrap">facet_wrap</a></span>(<span class="st">"obs"</span>, <span class="dt">scales =</span> <span class="st">"fixed"</span>, <span class="dt">ncol =</span> <span class="dv">4</span>)</code></pre></div>
<p><img src="loo2-non-factorizable_files/figure-html/yplots-1.png" width="95%" style="display: block; margin: auto;"></p>
<p>In the final step, we compute the ELPD based on the exact LOO-CV and compare it to the approximate PSIS-LOO result computed earlier.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_mean_exp &lt;-<span class="st"> </span><span class="cf">function</span>(x) {
  <span class="co"># more stable than log(mean(exp(x)))</span>
  max_x &lt;-<span class="st"> </span><span class="kw">max</span>(x)
  max_x <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">sum</span>(<span class="kw">exp</span>(x <span class="op">-</span><span class="st"> </span>max_x))) <span class="op">-</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">length</span>(x))
}
exact_elpds &lt;-<span class="st"> </span><span class="kw">apply</span>(loglik, <span class="dv">2</span>, log_mean_exp)
exact_elpd &lt;-<span class="st"> </span><span class="kw">sum</span>(exact_elpds)
<span class="kw">round</span>(exact_elpd, <span class="dv">1</span>)</code></pre></div>
<pre><code>[1] -188.4</code></pre>
<p>The results of the approximate and exact LOO-CV are similar but not as close as we would expect if there were no problematic observations. We can investigate this issue more closely by plotting the approximate against the exact pointwise ELPD values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">approx_elpd =</span> psis_loo<span class="op">$</span>pointwise[, <span class="st">"elpd_loo"</span>],
  <span class="dt">exact_elpd =</span> exact_elpds
)
<span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/ggplot">ggplot</a></span>(df, <span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/aes">aes</a></span>(<span class="dt">x =</span> approx_elpd, <span class="dt">y =</span> exact_elpd)) <span class="op">+</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/geom_abline">geom_abline</a></span>(<span class="dt">color =</span> <span class="st">"gray30"</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/geom_point">geom_point</a></span>(<span class="dt">size =</span> <span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/geom_point">geom_point</a></span>(<span class="dt">data =</span> df[<span class="dv">4</span>, ], <span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">color =</span> <span class="st">"red3"</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/labs">xlab</a></span>(<span class="st">"Approximate elpds"</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/labs">ylab</a></span>(<span class="st">"Exact elpds"</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/ggplot2/topics/coord_fixed">coord_fixed</a></span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">16</span>, <span class="op">-</span><span class="dv">3</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">16</span>, <span class="op">-</span><span class="dv">3</span>))</code></pre></div>
<p><img src="loo2-non-factorizable_files/figure-html/elpd-plot-1.png" width="60%" style="display: block; margin: auto;"></p>
<p>In the plot above the fourth data point —the observation flagged as problematic by the PSIS-LOO approximation— is colored in red and is the clear outlier. Otherwise, the correspondence between the exact and approximate values is strong. In fact, summing over the pointwise ELPD values and leaving out the fourth observation yields practically equivalent results for approximate and exact LOO-CV:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">without_pt_<span class="dv">4</span> &lt;-<span class="st"> </span><span class="kw">c</span>(
  <span class="dt">approx =</span> <span class="kw">sum</span>(psis_loo<span class="op">$</span>pointwise[<span class="op">-</span><span class="dv">4</span>, <span class="st">"elpd_loo"</span>]),
  <span class="dt">exact =</span> <span class="kw">sum</span>(exact_elpds[<span class="op">-</span><span class="dv">4</span>])  
)
<span class="kw">round</span>(without_pt_<span class="dv">4</span>, <span class="dv">2</span>)</code></pre></div>
<pre><code> approx   exact 
-172.97 -172.96 </code></pre>
<p>From this we can conclude that the difference we found when including <em>all</em> observations does not indicate a bug in our implementation of the approximate LOO-CV but rather a violation of its assumptions.</p>
</div>
</div>
</div>
<div id="working-with-stan-directly" class="section level1">
<h1 class="hasAnchor">
<a href="#working-with-stan-directly" class="anchor"></a>Working with Stan directly</h1>
<p>So far, we have specified the models in brms and only used Stan implicitely behind the scenes. This allowed us to focus on the primary purpose of validating approximate LOO-CV for non-factorizable models. However, we would also like to show how everything can be set up in Stan directly. The Stan code brms generates is human readable and so we can use it to learn some of the essential aspects of Stan and the particular model we are implementing. The Stan program below is a slightly modified version of the code extracted via <code><a href="http://www.rdocumentation.org/packages/brms/topics/stancode">stancode(fit_dummy)</a></code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">/</span><span class="er">/</span><span class="st"> </span>generated with brms <span class="fl">2.2</span>.<span class="dv">0</span>
functions {
<span class="op">/</span><span class="er">**</span><span class="st"> </span>
<span class="st"> </span><span class="er">*</span><span class="st"> </span>Normal log<span class="op">-</span>pdf <span class="cf">for</span> spatially lagged responses
 <span class="op">*</span><span class="st"> </span>
<span class="st"> </span><span class="er">*</span><span class="st"> </span><span class="er">@</span>param y Vector of response values.
 <span class="op">*</span><span class="st"> </span><span class="er">@</span>param mu Mean parameter vector.
 <span class="op">*</span><span class="st"> </span><span class="er">@</span>param sigma Positive scalar residual standard deviation.
 <span class="op">*</span><span class="st"> </span><span class="er">@</span>param rho Positive scalar autoregressive parameter.
 <span class="op">*</span><span class="st"> </span><span class="er">@</span>param W Spatial weight matrix.
 <span class="op">*</span>
<span class="st"> </span><span class="er">*</span><span class="st"> </span><span class="er">@</span>return A scalar to be added to the log posterior.
 <span class="op">*</span><span class="er">/</span>
<span class="st">  </span>real <span class="kw">normal_lagsar_lpdf</span>(vector y, vector mu, real sigma,
                          real rho, matrix W) {
    int N =<span class="st"> </span><span class="kw">rows</span>(y);
    real inv_sigma2 =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">square</span>(sigma);
    matrix[N, N] W_tilde =<span class="st"> </span><span class="op">-</span>rho <span class="op">*</span><span class="st"> </span>W;
    vector[N] half_pred;
    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) W_tilde[n, n] <span class="op">+</span><span class="er">=</span><span class="st"> </span><span class="dv">1</span>;
    half_pred =<span class="st"> </span>W_tilde <span class="op">*</span><span class="st"> </span>(y <span class="op">-</span><span class="st"> </span><span class="kw">mdivide_left</span>(W_tilde, mu));
    return <span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">log_determinant</span>(<span class="kw">crossprod</span>(W_tilde) <span class="op">*</span><span class="st"> </span>inv_sigma2) <span class="op">-</span>
<span class="st">           </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">dot_self</span>(half_pred) <span class="op">*</span><span class="st"> </span>inv_sigma2;
  }
}
data {
  int<span class="op">&lt;</span>lower=<span class="dv">1</span><span class="op">&gt;</span><span class="st"> </span>N;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>total number of observations
  vector[N] Y;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>response variable
  int<span class="op">&lt;</span>lower=<span class="dv">0</span><span class="op">&gt;</span><span class="st"> </span>Nmi;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>number of missings
  int<span class="op">&lt;</span>lower=<span class="dv">1</span><span class="op">&gt;</span><span class="st"> </span>Jmi[Nmi];  <span class="op">/</span><span class="er">/</span><span class="st"> </span>positions of missings
  int<span class="op">&lt;</span>lower=<span class="dv">1</span><span class="op">&gt;</span><span class="st"> </span>K;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>number of population<span class="op">-</span>level effects
  matrix[N, K] X;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>population<span class="op">-</span>level design matrix
  matrix[N, N] W;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>spatial weight matrix
  int prior_only;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>should the likelihood be ignored?
}
transformed data {
  int Kc =<span class="st"> </span>K <span class="op">-</span><span class="st"> </span><span class="dv">1</span>;
  matrix[N, K <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] Xc;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>centered version of X
  vector[K <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] means_X;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>column means of X before centering
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>K) {
    means_X[i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span><span class="kw">mean</span>(X[, i]);
    Xc[, i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span>X[, i] <span class="op">-</span><span class="st"> </span>means_X[i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>];
  }
}
parameters {
  vector[Nmi] Ymi;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>estimated missings
  vector[Kc] b;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>population<span class="op">-</span>level effects
  real temp_Intercept;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>temporary intercept
  real<span class="op">&lt;</span>lower=<span class="dv">0</span><span class="op">&gt;</span><span class="st"> </span>sigma;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>residual SD
  real<span class="op">&lt;</span>lower=<span class="dv">0</span>,upper=<span class="dv">1</span><span class="op">&gt;</span><span class="st"> </span>lagsar;  <span class="op">/</span><span class="er">/</span><span class="st"> </span>SAR parameter
}
transformed parameters {
}
model {
  vector[N] Yl =<span class="st"> </span>Y;
  vector[N] mu =<span class="st"> </span>Xc <span class="op">*</span><span class="st"> </span>b <span class="op">+</span><span class="st"> </span>temp_Intercept;
  Yl[Jmi] =<span class="st"> </span>Ymi;
  <span class="op">/</span><span class="er">/</span><span class="st"> </span>priors including all constants
  target <span class="op">+</span><span class="er">=</span><span class="st"> </span><span class="kw">student_t_lpdf</span>(temp_Intercept <span class="op">|</span><span class="st"> </span><span class="dv">3</span>, <span class="dv">34</span>, <span class="dv">17</span>);
  target <span class="op">+</span><span class="er">=</span><span class="st"> </span><span class="kw">student_t_lpdf</span>(sigma <span class="op">|</span><span class="st"> </span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">17</span>)
    <span class="op">-</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span><span class="kw">student_t_lccdf</span>(<span class="dv">0</span> <span class="op">|</span><span class="st"> </span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">17</span>);
  <span class="op">/</span><span class="er">/</span><span class="st"> </span>likelihood including all constants
  <span class="cf">if</span> (<span class="op">!</span>prior_only) {
    target <span class="op">+</span><span class="er">=</span><span class="st"> </span><span class="kw">normal_lagsar_lpdf</span>(Yl <span class="op">|</span><span class="st"> </span>mu, sigma, lagsar, W);
  }
}
generated quantities {
  <span class="op">/</span><span class="er">/</span><span class="st"> </span>actual population<span class="op">-</span>level intercept
  real b_Intercept =<span class="st"> </span>temp_Intercept <span class="op">-</span><span class="st"> </span><span class="kw">dot_product</span>(means_X, b);
}</code></pre></div>
<p>Here we want to focus on two aspects of the Stan code. First, because there is no built-in function in Stan that calculates the log-likelihood for the lag-SAR model, we define a new <code>normal_lagsar_lpdf</code> function in the <code>functions</code> block of the Stan program. This is the same function we showed earlier in the vignette and it can be used to compute the log-likelihood in an efficient and numerically stable way. The <code>_lpdf</code> suffix used in the function name informs Stan that this is a log probability density function.</p>
<p>Second, this Stan program nicely illustrates how to set up missing value imputation. Instead of just computing the log-likelihood for the observed responses <code>Y</code>, we define a new variable <code>Yl</code> which is equal to <code>Y</code> if the reponse is observed and equal to <code>Ymi</code> if the response is missing. The latter is in turn defined as a parameter and thus estimated along with all other paramters of the model. More details about missing value imputation in Stan can be found in the <em>Missing Data &amp; Partially Known Parameters</em> section of the <a href="http://mc-stan.org/users/documentation/index.html">Stan manual</a>.</p>
<p>The Stan code extracted from brms is not only helpful when learning Stan, but can also drastically speed up the specification of models that are not support by brms. If brms can fit a model similar but not identical to the desired model, we can let brms generate the Stan program for the similar model and then mold it into the program that implements the model we actually want to fit. Rather than calling <code><a href="http://www.rdocumentation.org/packages/brms/topics/stancode">stancode()</a></code>, which requires an existing fitted model object, we recommend using <code><a href="http://www.rdocumentation.org/packages/brms/topics/make_stancode">make_stancode()</a></code> and specifying the <code>save_model</code> argument to write the Stan program to a file. The corresponding data can be prepared with <code><a href="http://www.rdocumentation.org/packages/brms/topics/make_standata">make_standata()</a></code> and then manually amended if needed. Once the code and data have been edited, they can be passed to RStan’s <code>stan()</code> function via the <code>file</code> and <code>data</code> arguments.</p>
</div>
<div id="conclusion" class="section level1">
<h1 class="hasAnchor">
<a href="#conclusion" class="anchor"></a>Conclusion</h1>
<p>In summary, we have shown how to set up and validate approximate and exact LOO-CV for non-factorizable multivariate normal models using Stan with the <strong>brms</strong> and <strong>loo</strong> packages. Although we focused on the particular example of a spatial SAR model, the presented recipe applies more generally to models that can be expressed in terms of a multivariate normal likelihood.</p>
<p><br></p>
</div>
<div id="references" class="section level1">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<p>Anselin L. (1988). <em>Spatial econometrics: methods and models</em>. Dordrecht: Kluwer Academic.</p>
<p>Sundararajan S. &amp; Keerthi S. S. (2001). Predictive approaches for choosing hyperparameters in Gaussian processes. <em>Neural Computation</em>, 13(5), 1103–1118.</p>
<p>Vehtari A., Mononen T., Tolvanen V., Sivula T., &amp; Winther O. (2016). Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models. <em>Journal of Machine Learning Research</em>, 17(103), 1–38. <a href="http://jmlr.org/papers/v17/14-540.html">Online</a>.</p>
<p>Vehtari A., Gelman A., &amp; Gabry J. (2017a). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. <em>Statistics and Computing</em>, 27(5), 1413–1432. <a href="doi:10.1007/s11222-016-9696-4" class="uri">doi:10.1007/s11222-016-9696-4</a>. <a href="http://link.springer.com/article/10.1007/s11222-016-9696-4">Online</a>. <a href="https://arxiv.org/abs/1507.04544">arXiv preprint arXiv:1507.04544</a>.</p>
<p>Vehtari A., Gelman A., &amp; Gabry J. (2017b). Pareto smoothed importance sampling. <a href="https://arxiv.org/abs/1507.02646">arXiv preprint arXiv:1507.02646</a>.</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#introduction">Introduction</a></li>
      <li>
<a href="#loo-cv-for-multivariate-normal-models">LOO-CV for multivariate normal models</a><ul class="nav nav-pills nav-stacked">
<li><a href="#approximate-loo-cv-using-integrated-importance-sampling">Approximate LOO-CV using integrated importance-sampling</a></li>
      <li><a href="#exact-loo-cv-with-re-fitting">Exact LOO-CV with re-fitting</a></li>
      </ul>
</li>
      <li>
<a href="#lagged-sar-models">Lagged SAR models</a><ul class="nav nav-pills nav-stacked">
<li><a href="#case-study-neighborhood-crime-in-columbus-ohio">Case Study: Neighborhood Crime in Columbus, Ohio</a></li>
      </ul>
</li>
      <li><a href="#working-with-stan-directly">Working with Stan directly</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by .</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
