---
title: "Using Leave-one-out cross-validation for large data"
author: "Mans Magnusson, Paul BÃ¼rkner and Aki Vehtari"
date: "`r Sys.Date()`"
output:
  html_vignette:
    toc: yes
params:
  EVAL: !r identical(Sys.getenv("NOT_CRAN"), "true")
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Writing Stan programs for use with the loo package}
-->

# Introduction

This vignette demonstrates how to do leave-one-out cross-validation for large data
using the __loo__ package and Stan. Some sections from this vignette are excerpted from the papers

* Magnusson, M., Riis Andersen, M., Jonasson, J. and Vehtari, A. (2019). Leave-One-Out Cross-Validation for Model Comparison in Large Data.

* Magnusson, M., Riis Andersen, M., Jonasson, J. and Vehtari, A. (2019). Leave-One-Out Cross-Validation for Large Data. In _International Conference on Machine Learning_ 

* Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. _Statistics and Computing_. 27(5), 1413--1432. \doi:10.1007/s11222-016-9696-4. Links: [published](http://link.springer.com/article/10.1007\%2Fs11222-016-9696-4) | [arXiv preprint](http://arxiv.org/abs/1507.04544).

* Vehtari, A., Gelman, A., and Gabry, J. (2017). Pareto smoothed importance sampling. [arXiv preprint arXiv:1507.04544](http://arxiv.org/abs/1507.04544).

which provide important background for understanding the methods implemented in
the package.


# Example: Well water in Bangladesh

We use the same example as in previous loo vignettes, namely the well water example. See the vignette [here](http://www.google.com) for a more detail description. 

The sample size in this example is $N=3020$, which is not very large, but it serves
as an example of how to use subsample LOO for very efficient model inference. 

## Coding the Stan model

Here is the Stan code for fitting the logistic regression model, which 
we save in a file called `logistic.stan`:

```
data {
  int<lower=0> N;             // number of data points
  int<lower=0> P;             // number of predictors (including intercept)
  matrix[N,P] X;              // predictors (including 1s for intercept)
  int<lower=0,upper=1> y[N];  // binary outcome
}
parameters {
  vector[P] beta;
}
model {
  beta ~ normal(0, 1);
  y ~ bernoulli_logit(X * beta);
}
```

Note that we, unlike the general approach, do not compute the log-likelihood for 
each observation in the Stan code. The reason is that computing the log-likelihood
values for all observations is not necessary and will take up a lot of memory,
especially for very large data. Instead, we define the log-likelihood as an R function 
as follows. 


```{r}
llfun_logistic <- function(data_i, draws) {
  x_i <- as.matrix(data_i[, which(grepl(colnames(data_i), pattern = "X")), drop=FALSE])
  y_pred <- draws %*% t(x_i)
  dbinom(x = data_i$y, size = 1, prob = 1  / (1 + exp(-y_pred)), log = TRUE)
}
```

The function ```llfun_logistic()``` need to be a function with argument ```data_i``` and ```draws```. We can see if the function is working by using the ```loo_i()``` function (see below).


## Fitting the model with RStan

Next we fit the model in Stan using the **rstan** package:

```{r, eval=FALSE}
library("rstan")
set.seed(4711)

# Prepare data
url <- "http://stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat"
wells <- read.table(url)
wells$dist100 <- with(wells, dist / 100)
X <- model.matrix(~ dist100 + arsenic, wells)
standata <- list(y = wells$switch, X = X, N = nrow(X), P = ncol(X))

# Fit model
fit_1 <- stan("logistic.stan", data = standata, seed = 4711)
print(fit_1, pars = "beta")
```

```
         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
beta[1]  0.00       0 0.08 -0.15 -0.05  0.00  0.06  0.16  1933    1
beta[2] -0.89       0 0.10 -1.09 -0.96 -0.89 -0.82 -0.69  2332    1
beta[3]  0.46       0 0.04  0.38  0.43  0.46  0.49  0.54  2051    1
```

We can now simply test that the log-likelihood function is working as it should be able to be used in `loo`.

```{r, eval=FALSE}
parameter_draws <- extract(fit_1)$beta
stan_df <- as.data.frame(standata)
loo_i(1, llfun_logistic, data = stan_df, draws = parameter_draws)
```

```
$pointwise
    elpd_loo mcse_elpd_loo        p_loo     looic
1 -0.3310342  0.0002908997 0.0003487243 0.6620683
...
```

## Computing approximate leave-one-out cross-validation using PSIS-LOO and subsampling

We can then use the **loo** package to compute the efficient PSIS-LOO
approximation to exact LOO-CV using subsampling:

```{r, eval=FALSE}
library("loo")
parameter_draws <- extract(fit_1)$beta
stan_df <- as.data.frame(standata)

set.seed(4711)
looss_1 <- loo_subsample(llfun_logistic, draws = parameter_draws, data = stan_df, observations = 100)
print(looss_1)
```

```
Computed from 4000 by 100 subsampled log-likelihood
values from 3020 total observations.

         Estimate   SE subsampling SE
elpd_loo  -1968.5 15.6            0.3
p_loo         3.1  0.1            0.4
looic      3936.9 31.2            0.6
------
Monte Carlo SE of elpd_loo is 0.0.

All Pareto k estimates are good (k < 0.5).
See help('pareto-k-diagnostic') for details.
```

The function creates an object of class `psis_loo_ss`, that inherits from `psis_loo, loo`.

The printed output from the `loo` function shows the estimates
$\widehat{\mbox{elpd}}_{\rm loo}$ (expected log predictive density),
$\widehat{p}_{\rm loo}$ (effective number of parameters), and ${\rm looic} =-2\,
\widehat{\mbox{elpd}}_{\rm loo}$ (the LOO information criterion). In addition, 
subsampling SE contain the additional uncertainty due to the subsampling used. 

The line at the bottom of the printed output provides information about the
reliability of the LOO approximation (the interpretation of the $k$ parameter is
explained in `help('pareto-k-diagnostic')` and in greater detail
in Vehtari, Gelman, and Gabry (2017)). In this case, the message tells us that
all of the estimates for $k$ are fine _for this given subsample_.


### Adding additional subsamples

If we are not satisfied with the subsample size (ie the accuracy) we can simply add more 
samples until we are satisfied with the accuracy using `update()`.

```{r, eval=FALSE}
set.seed(4711)
looss_1b <- update(looss_1, draws = parameter_draws, data = stan_df, observations = 200)
print(looss_1b)
```

```
Computed from 4000 by 200 subsampled log-likelihood
values from 3020 total observations.

         Estimate   SE subsampling SE
elpd_loo  -1968.3 15.6            0.2
p_loo         3.2  0.1            0.4
looic      3936.7 31.2            0.5
------
Monte Carlo SE of elpd_loo is 0.0.

All Pareto k estimates are good (k < 0.5).
See help('pareto-k-diagnostic') for details.
```

### Specifying estimator and sampling method

The performance relies on two parts, the estimation method, and the approximation used for the elpd. See the documentation for more information on what estimators and approximations that are implemented. The default implementation is using the point log predictive density evaluated at the mean of the posterior (plpd) and the difference estimator (diff). This combination has a focus on fast inference. We can easily use other estimators as well as other elpd approximations.

```{r, eval=FALSE}
set.seed(4711)
looss_2 <- loo_subsample(x = llfun_logistic, draws = parameter_draws, data = stan_df, observations = 100, estimator = "hh_pps", loo_approximation = "lpd", loo_approximation_draws = 100)
print(looss_2)
```


```
Computed from 4000 by 100 subsampled log-likelihood
values from 3020 total observations.

         Estimate   SE subsampling SE
elpd_loo  -1968.9 15.4            0.5
p_loo         3.5  0.2            0.5
looic      3937.9 30.7            1.1
------
Monte Carlo SE of elpd_loo is 0.0.

All Pareto k estimates are good (k < 0.5).
See help('pareto-k-diagnostic') for details.
```

See the documentation and references for details on implemented approximations.


## Using posterior approximations

Using posterior approximations, such as variational inference and Laplace approximations can further speed-up LOO-CV for large data. Here we use Laplace approximation in Stan.

```{r, eval=FALSE}
library("rstan")
fit_laplace <- optimizing(fit_1, data = standata, draws = 2000)
print(fit_laplace, pars = "beta")
parameter_draws <- extract(fit_laplace)
log_p <- fit_laplace$log_p # The log density of the posterior
log_g <- fit_laplace$log_g # The log density of the approximation
```


Using the posterior approximation we can then do LOO-CV by correcting for the
posterior approximation when we compute the elpd

```{r, eval=FALSE}
set.seed(4711)
aploo_1 <- loo_approximate_posterior(x = llfun_logistic, draws = parameter_draws, data = stan_df, log_p = log_p, log_g = log_g)
print(aploo_1)
```

The function creates a class, `psis_loo_ap` that inherits from `psis_loo, loo`.

```
Computed from 2000 by 3020 log-likelihood matrix

         Estimate   SE
elpd_loo  -1968.4 15.6
p_loo         3.2  0.2
looic      3936.8 31.2
------
Posterior approximation correction used.
Monte Carlo SE of elpd_loo is 0.0.

Pareto k diagnostic values:
                         Count Pct.    Min. n_eff
(-Inf, 0.5]   (good)     2989  99.0%   1827      
 (0.5, 0.7]   (ok)         31   1.0%   1996      
   (0.7, 1]   (bad)         0   0.0%   <NA>      
   (1, Inf)   (very bad)    0   0.0%   <NA>      

All Pareto k estimates are ok (k < 0.7).
See help('pareto-k-diagnostic') for details.
```

Posterior approximation correction can also be used together with subsampling. 

```{r, eval=FALSE}
set.seed(4711)
looapss_1 <- loo_subsample(x = llfun_logistic, draws = parameter_draws, data = stan_df, log_p = log_p, log_g = log_g, observations = 100)
print(looapss_1)
```

```
Computed from 2000 by 100 subsampled log-likelihood
values from 3020 total observations.

         Estimate   SE subsampling SE
elpd_loo  -1968.2 15.6            0.4
p_loo         2.9  0.1            0.5
looic      3936.4 31.1            0.8
------
Posterior approximation correction used.
Monte Carlo SE of elpd_loo is 0.0.

Pareto k diagnostic values:
                         Count Pct.    Min. n_eff
(-Inf, 0.5]   (good)     97    97.0%   1971      
 (0.5, 0.7]   (ok)        3     3.0%   1997      
   (0.7, 1]   (bad)       0     0.0%   <NA>      
   (1, Inf)   (very bad)  0     0.0%   <NA>      

All Pareto k estimates are ok (k < 0.7).
See help('pareto-k-diagnostic') for details.
```

The object created is of a `psis_loo_ss` class that inherits from a `psis_loo_ap` class that, in turn, inherits from `psis_loo, loo`.


## Comparing models

To compare this model to an alternative model for the same data we can use the 
`loo_compare` function in the `loo` package. First we'll fit a second model to the well-switching data, using `log(arsenic)` instead of `arsenic` as a predictor:

```{r, eval=FALSE}
standata$X[, "arsenic"] <- log(standata$X[, "arsenic"])
fit_2 <- stan(fit = fit_1, data = standata) 
parameter_draws_2 <- extract(fit2)

looss_2 <- loo_subsample(x = llfun_logistic, draws = parameter_draws_2, data = stan_df, observations = 100)

print(looss_2)
```

```
Computed from 4000 by 100 subsampled log-likelihood
values from 3020 total observations.

         Estimate   SE subsampling SE
elpd_loo  -1952.0 16.2            0.2
p_loo         2.6  0.1            0.3
looic      3903.9 32.4            0.4
------
Monte Carlo SE of elpd_loo is 0.0.

All Pareto k estimates are good (k < 0.5).
See help('pareto-k-diagnostic') for details.
```

We can now compare the models on LOO using the `loo_compare` function:

```{r, eval=FALSE}
# Compare
comp <- loo_compare(looss_1, looss_2)
print(comp)
```

```
Warning: Different subsamples in 'model2' and 'model1'. Naive diff SE is used.

       elpd_diff se_diff subsampling_se_diff
model2  0.0       0.0     0.0               
model1 16.5      22.5     0.4               
```

This new object, `compare.loo_ss`, contains the estimated difference of expected 
leave-one-out prediction errors between the two models, along with the standard 
error. As the warning indicates, just using to subsamples can be used for comparison, but this will not take the correlations between different observations into account. Here we see that the naive SE is
22.5 and we cannot see any difference in performance between the models.

To do this we can simply extract the observations used in `looss1` and use them in `looss2` by supplying the `looss1` object to `observations`.


```{r, eval=FALSE}
looss_2_m <- loo_subsample(x = llfun_logistic, draws = parameter_draws_2, data = stan_df, observations = looss1)
```

We can also supply the subsampling indices specifically using `obs_idx()` as

```{r, eval=FALSE}
idx <- obs_idx(looss1)
looss_2_m <- loo_subsample(x = llfun_logistic, draws = parameter_draws_2, data = stan_df, observations = idx)
```

```
Simple random sampling with replacement assumed.
```

This gives us the message that we assume these observations to have been sampled with simple random sampling, which they had when using ```diff_srs``` estimator in ```looss_1```.

We can now compare the models and estimate the difference more efficiently since they use the same subsampled observations.

```{r, eval=FALSE}
comp <- loo_compare(looss_1, looss_2_m)
print(comp, simplify=TRUE) 
```

```
       elpd_diff se_diff subsampling_se_diff
model2  0.0       0.0     0.0               
model1 16.1       4.4     0.1               
```

The first column shows the difference in ELPD relative to the model with 
the largest ELPD. In this case, the difference in `elpd` and its scale relative 
to the approximate standard error of the difference) indicates a 
preference for the second model (`model2`). Since the subsampling uncertainty
is so small it can effectively be ignored. If we need larger subsamples we can simply add samples as follows:

```{r, eval=FALSE}
looss_1 <- loo_subsample(x = looss_1, draws = parameter_draws_2, data = stan_df, observations = 100)
looss_2_m <- loo_subsample(x = looss_2_m, draws = parameter_draws_2, data = stan_df, observations = looss1)
comp <- loo_compare(looss_1, looss_2_m)
print(comp, simplify=TRUE) 
```

```
       elpd_diff se_diff subsampling_se_diff
model2  0.0       0.0     0.0               
model1 16.3       4.4     0.1
```

Although, in this case, adding new observations does not gain much.

It is also possible to compare a subsampled loo computation with a full loo object.

```{r, eval=FALSE}
looss_2_full <- loo(x = llfun_logistic, draws = parameter_draws_2, data = stan_df2)
comp <- loo_compare(looss_1, looss_2_full)
print(comp, simplify=TRUE) 
```

```
Estimated elpd_diff using observations common to 'model2' and 'model1'.
```

We get the message that only the observations that are included in both model1 and model2 are included in the computation. But for the full model,```psis``` is used as an approximation. 

```
       elpd_diff se_diff subsampling_se_diff
model2  0.0       0.0     0.0               
model1 16.3       4.4     0.3   
```

Again, we do not gain much, but we actually lose some. This is due to a technical detail not elaborated here. Although in general, the difference should be better or negligible.

# References

Gelman, A., and Hill, J. (2007).  *Data Analysis Using Regression and Multilevel Hierarchical Models.*  Cambridge University Press.

Stan Development Team (2017). _The Stan C++ Library, Version 2.17.0._   http://mc-stan.org

Stan Development Team (2018) _RStan: the R interface to Stan, Version 2.17.3._   http://mc-stan.org

Magnusson, M., Riis Andersen, M., Jonasson, J. and Vehtari, A. (2019). Leave-One-Out Cross-Validation for Model Comparison in Large Data.

Magnusson, M., Riis Andersen, M., Jonasson, J. and Vehtari, A. (2019). Leave-One-Out Cross-Validation for Large Data. In _International Conference on Machine Learning_ 

Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC. _Statistics and
Computing_. 27(5), 1413--1432. \doi:10.1007/s11222-016-9696-4.
[online](http://link.springer.com/article/10.1007\%2Fs11222-016-9696-4), 
[arXiv preprint arXiv:1507.04544](http://arxiv.org/abs/1507.04544).

Vehtari, A., Gelman, A., and Gabry, J. (2017). Pareto smoothed importance
sampling. [arXiv preprint arXiv:1507.02646](http://arxiv.org/abs/1507.02646).
