---
title: "Using the loo package"
author: "Jonah Gabry, Ben Goodrich, and Aki Vehtari"
date: "`r Sys.Date()`"
output:
  html_vignette:
    toc: yes
params:
  EVAL: !r identical(Sys.getenv("NOT_CRAN"), "true")
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{loo: Using the loo package}
-->
```{r, child="children/SETTINGS-knitr.txt"}
```
```{r, child="children/SETTINGS-gg.txt"}
```
```{r, child="children/SETTINGS-rstan.txt"}
```
```{r, child="children/SETTINGS-loo.txt"}
```

# Introduction

This vignette demonstrates Pareto smoothed importance-sampling
leave-one-out cross-validation implemented in the __loo__ package.

PSIS-LOO, the Pareto $k$ diagnostic, and the effective sample size
estimates for PSIS-LOO are presented in

Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC. _Statistics and
Computing_. 27(5), 1413--1432. doi:10.1007/s11222-016-9696-4.
Links: [published](http://link.springer.com/article/10.1007\%2Fs11222-016-9696-4) | 
[arXiv preprint](http://arxiv.org/abs/1507.04544).

and

Vehtari, A., Gelman, A., and Gabry, J. (2017). Pareto smoothed importance sampling. [arXiv preprint arXiv:1507.04544](http://arxiv.org/abs/1507.04544).

```{r, child="children/four_steps.txt"}
```

This vignette focuses on Step 3 using the __loo__ package. The Poisson and negative binomial regression models and `stan_glm` used in the example are covered in more depth by the vignette entitled [Estimating Generalized Linear Models for Count Data with rstanarm](https://cran.r-project.org/web/packages/rstanarm/vignettes/count.html)

# Poisson and Negative Binomial Regression Example

The example data comes from Chapter 8.3 of [Gelman and Hill (2007)](http://www.stat.columbia.edu/~gelman/arm/)

We want to make inferences about the efficacy of a certain pest management system at reducing the number of roaches in urban apartments. Here is how Gelman and Hill describe the experiment (pg. 161):

> the treatment and control were applied to 160 and 104 apartments, respectively, and the outcome measurement $y_i$ in each apartment $i$ was the number of roaches caught in a set of traps. Different apartments had traps for different numbers of days

In addition to an intercept, the regression predictors for the model are the pre-treatment number of roaches `roach1`, the treatment indicator `treatment`, and a variable indicating whether the apartment is in a building restricted to elderly residents `senior`. Because the number of days for which the roach traps were used is not the same for all apartments in the sample, we include it as an `exposure2` by adding $\ln(u_i)$) to the linear predictor $\eta_i$ and it can be specified using the `offset` argument to `stan_glm`.

```{r, count-roaches-mcmc, results="hide"}
library(rstanarm)
library(loo)
library(bayesplot)
data(roaches)

# Rescale
roaches$roach1 <- roaches$roach1 / 100
# Estimate with stan_glm
stan_glm1 <- stan_glm(y ~ roach1 + treatment + senior, offset = log(exposure2),
                      data = roaches, family = poisson, 
                      prior = normal(0,2.5), prior_intercept = normal(0,5),
                      chains = CHAINS, cores = CORES, seed = SEED)
```

Usually we would also run posterior predictive checks as shown in
[Estimating Generalized Linear Models for Count Data with
rstanarm](https://cran.r-project.org/web/packages/rstanarm/vignettes/count.html).
Here we focus on methods provided by __loo__ package.

Although cross-validation is mostly used for model comparison, we
start demonstarting how it is useful also in model checking.

We start by computing PSIS-LOO with `loo` function. We use `save_psis
= TRUE` to save some intermediate computation results to be re-used
later.
```{r}
loo1<-loo(stan_glm1, save_psis = TRUE)
```

`loo` gives us warnings from Pareto diagnostics which indicates that
leave-one-out posteriors are so much different from te full posterior
that importance-sampling is not able to correct the difference.
We can see more details by printing the loo-object.
```{r}
loo1
```

We can see the proportion of leave-one-out folds having high $k$
values in different categories indicating the reliability of the
estimates. In addition, the minimum of the effective sample in that
category is shown to give idea why higher $k$ values are bad. As some
$k>1$, we are not able to compute estimate for the Monte Carlo
standard error (SE) of elpd_loo and NA is displayed. elpd_loo estimate
can be considered to be very bad. In case of well specified model we
would expect to see p_loo smaller than or similar to the total number
of parameters in the model. Here p_loo is about 292 which about 70
times higher than the total number of of parameteres in the model
indicating also bad model misspecification.

We can plot $k$ values in the order of data with vertical lines
corresponding to the same categories as above.

```{r}
plot(loo1)
```

This plot is useful to quickly see the distribution of $k$ values, but
also to see possible structure with respect to data ordering. Now
there seems to be a block of data which is easier to predict (indeces
around 90--150), but also for these data we see some high $k$ values.

`loo` package can be used in combination with `bayesplot` package for
leave-one-out cross-validation marginal posterior predictive checks
[Gabry et al (2018)](http://arxiv.org/abs/1709.01449). LOO-PIT values
are cumulative probabilities for $y_i$ computed using LOO marginal
predictive distributions $p(y_i|y_{-i})$. For a good model, the
distribution of LOO-PIT values should be uniform and the following
plot shows as a reference smoothed distribution estimates for several
independently generated uniform random values.

```{r}
yrep <- posterior_predict(stan_glm1)
ppc_loo_pit_overlay(roaches$y, yrep, lw = weights(loo1$psis_object))
```

Excessive amount of values close to 0 and 1, indicate that the model
is under-dispersed compared to data, and we should consider a model
that can have larger dispersion.

Here we will try [negative
binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution)
regression, which is commonly used for overdispersed count data.  The
negative binomial distribution allows the (conditional) mean and
variance of $y$ to differ unlike the Poisson distribution.

```{r, count-roaches-negbin, results="hide"}
stan_glm2 <- update(stan_glm1, family = neg_binomial_2) 
```

```{r}
loo2<-loo(stan_glm2, save_psis = TRUE)
```

We get again a warning, and check more details.

```{r}
#this does not work
#loo2plus<-loo(stan_glm2, k_threshold = 0.7)
```

```{r}
loo2
```

Now there is only one bad $k$ value, which however is still in a such
range that we get a useful estimate for the Monte Carlo SE of
elpd_loo, which is small compared to other uncertainties. p_loo is
about 7 which is a bit higher than the total number of of parameteres
in the model which is possible with well specified models, but
indicates also slight model misspecification.

Next we look at the plot of $k$ values.
```{r}
plot(loo2)
```

Most of the $k$ values are small, but we see one much higher $k$ value
which sometimes indicates an ``outlier''. Although high $k$ values
often indicate model misspecification, small $k$ values do not
mean that model is well specified.

For further model checking we examine LOO-PIT values.
```{r}
yrep <- posterior_predict(stan_glm2)
ppc_loo_pit_overlay(roaches$y, yrep, lw = weights(loo2$psis_object))
```

Right side of the plot looks good, but the left side indicates too
much mass for small values and the model is likely to be misspecified.
A reasoanble guess is that that a hurdle or zero-inflated model is
needed.

```{r, count-roaches-loo}
compare_models(loo1, loo2)
```

The difference is much larger than twice the estimated standard
error. The negative-binomial model is better than Poisson model, but
still it is far from perfect model as shown by LOO-PIT.  We leave a
hurdle model for another case study.

# References

Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., and Gelman,
A. (2018). Visualization in Bayesian workflow. Journal of the Royal
Statistical Society Series A, accepted fo publication. [arXiv preprint
arXiv:1709.01449](http://arxiv.org/abs/1709.01449)

Gelman, A. and Hill, J. (2007). _Data Analysis Using Regression and
Multilevel/Hierarchical Models._ Cambridge University Press, Cambridge, UK.

Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC. _Statistics and
Computing_. 27(5), 1413--1432. doi:10.1007/s11222-016-9696-4.
[online](http://link.springer.com/article/10.1007\%2Fs11222-016-9696-4), 
[arXiv preprint arXiv:1507.04544](http://arxiv.org/abs/1507.04544).

Vehtari, A., Gelman, A., and Gabry, J. (2017). Pareto smoothed importance sampling. [arXiv preprint arXiv:1507.02646](http://arxiv.org/abs/1507.02646).
